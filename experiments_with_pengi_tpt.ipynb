{
 "cells":[
  {
   "cell_type":"markdown",
   "source":[
    "# Sheet"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"Sheet",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import os\n",
    "\n",
    "os.chdir(\"\/data\/notebook_files\/Pengi\")"
   ],
   "execution_count":3,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"CEmny4RGeRi3gCRQ5HhaHK",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from Pengi.wrapper import PengiWrapper as Pengi\n",
    "\n",
    "pengi = Pengi(config=\"base\")\n",
    "\n",
    "generated_responses = pengi.generate(audio_paths=[\"\/data\/notebook_files\/violin_0.wav\"],\n",
    "                                   text_prompts=[\"generate audio caption \"],\n",
    "                                   add_texts=[\",\"],\n",
    "                                   max_len=30,\n",
    "                                   beam_size=6,\n",
    "                                   temperature=1.0,\n",
    "                                   stop_token=' <|endoftext|>'\n",
    "                                   )"
   ],
   "execution_count":4,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"mj4GFFArORgzUqLpkqj4Xn",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "pengi_output_prompts = generated_responses[0][0]\n",
    "pengi_output_prompts"
   ],
   "execution_count":8,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"vCOTMpO2xa7elXVBy5Cwd6",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## DO NOT EDIT CELL BELOW\n",
    "\n",
    "This codeblock below is our \"ground truth\" for reference."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"am6VFcxF6g7eEGZsACmwgc",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\"\"\"make variations of input image\"\"\"\n",
    "import os, sys\n",
    "os.chdir(\"\/data\/notebook_files\/Make-An-Audio-main\")\n",
    "\n",
    "print(os.path.abspath(\".\"))\n",
    "import argparse, os, sys, glob\n",
    "import PIL\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange, repeat\n",
    "from torchvision.utils import make_grid\n",
    "from torch import autocast\n",
    "import librosa\n",
    "# from contextlib import nullcontext\n",
    "import time\n",
    "from pytorch_lightning import seed_everything\n",
    "import math\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from vocoder.bigvgan.models import VocoderBigVGAN\n",
    "# from ldm.data.extract_mel_spectrogram import TRANSFORMS_22050,TRANSFORMS_16000\n",
    "from preprocess.NAT_mel import MelNet\n",
    "import soundfile\n",
    "\n",
    "batch_max_length = 624\n",
    "SAMPLE_RATE= 16000\n",
    "\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=True):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_audio(path,transform,sr=16000,batch_max_length=624):# load wav and return mel\n",
    "    wav,_ = librosa.load(path,sr=sr)\n",
    "\n",
    "    audio = transform(wav) # (1,melbins,T)\n",
    "    if audio.shape[2] <= batch_max_length:\n",
    "        n_repeat = math.ceil((batch_max_length + 1) \/ audio.shape[1])\n",
    "        audio = audio.repeat(1,1, n_repeat)\n",
    "\n",
    "    audio = audio[..., :batch_max_length].unsqueeze(0) # shape [1,1,80,batch_max_length]\n",
    "    return audio\n",
    "\n",
    "def load_img(path):# load mel\n",
    "    audio = np.load(path)\n",
    "    if audio.shape[1] <= batch_max_length:\n",
    "        n_repeat = math.ceil((batch_max_length + 1) \/ audio.shape[1])\n",
    "        audio = np.tile(audio, reps=(1, n_repeat))\n",
    "\n",
    "    audio = audio[:, :batch_max_length]\n",
    "    audio = torch.FloatTensor(audio)[None, None, :, :] # [1,1,80,batch_max_length]\n",
    "    return audio\n",
    "\n",
    "\n",
    "class Opt:\n",
    "    def __init__(self):\n",
    "        self.config = \"\/data\/notebook_files\/Make-An-Audio-main\/configs\/text_to_audio\/txt2audio_args.yaml\"\n",
    "        self.ckpt = \"\/data\/notebook_files\/Make-An-Audio-main\/useful_ckpts\/maa1_full.ckpt\"\n",
    "        self.vocoder_ckpt = \"\/data\/notebook_files\/Make-An-Audio-main\/useful_ckpts\/bigvnat\"\n",
    "        self.outdir = \"\/data\/notebook_files\/audio-outputs-with-pengi\"\n",
    "        self.from_file = None\n",
    "        \n",
    "        self.strength = 0.3\n",
    "        self.seed = 42\n",
    "        self.scale = 3.0\n",
    "        self.n_samples = 2\n",
    "        self.n_iter = 1\n",
    "\n",
    "        self.ddim_steps = 100\n",
    "        self.ddim_eta = 0.0\n",
    "\n",
    "        self.prompt = \"a warm melodic sounding violin\"\n",
    "        self.init_audio = \"\/data\/notebook_files\/violin_0.wav\"\n",
    "        self.pengi_iterations = 5\n",
    "\n",
    "def main():\n",
    "    opt = Opt()\n",
    "    seed_everything(opt.seed)\n",
    "\n",
    "    config = OmegaConf.load(f\"{opt.config}\")\n",
    "    model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    hparams = {\n",
    "        'audio_sample_rate': SAMPLE_RATE,\n",
    "        'audio_num_mel_bins':80,\n",
    "        'fft_size': 1024,\n",
    "        'win_size': 1024,\n",
    "        'hop_size': 256,\n",
    "        'fmin': 0,\n",
    "        'fmax': 8000,\n",
    "        'batch_max_length': 1248, \n",
    "        'mode': 'pad', # pad,none,\n",
    "    }\n",
    "    melnet = MelNet(hparams)\n",
    "    sampler = DDIMSampler(model)\n",
    "    vocoder = VocoderBigVGAN(opt.vocoder_ckpt,device)\n",
    "\n",
    "    os.makedirs(opt.outdir, exist_ok=True)\n",
    "    outpath = opt.outdir\n",
    "\n",
    "    batch_size = opt.n_samples # 一个prompt产生n_samples个结果\n",
    "    if not opt.from_file: # load prompts from this file\n",
    "        prompt = opt.prompt\n",
    "        assert prompt is not None\n",
    "        data = [batch_size * [prompt]]\n",
    "    else:\n",
    "        print(f\"reading prompts from {opt.from_file}\")\n",
    "        with open(opt.from_file, \"r\") as f:\n",
    "            data = f.read().splitlines()\n",
    "            data = list(chunk(data, batch_size))\n",
    "\n",
    "\n",
    "    sample_path = os.path.join(outpath, \"samples\")\n",
    "    os.makedirs(sample_path, exist_ok=True)\n",
    "    base_count = len(os.listdir(sample_path))\n",
    "\n",
    "    assert os.path.isfile(opt.init_audio)\n",
    "    init_image = load_audio(opt.init_audio,transform=melnet).to(device)\n",
    "    init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "    init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
    "    sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
    "\n",
    "    assert 0. <= opt.strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "    t_enc = int(opt.strength * opt.ddim_steps)\n",
    "    print(f\"target t_enc is {t_enc} steps\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with model.ema_scope():\n",
    "            tic = time.time()\n",
    "            all_samples = list()\n",
    "            for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
    "                for prompts in tqdm(data, desc=\"data\"):\n",
    "                    uc = None\n",
    "                    if opt.scale != 1.0: # default=5.0\n",
    "                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                    if isinstance(prompts, tuple):\n",
    "                        prompts = list(prompts)\n",
    "                    c = model.get_learned_conditioning(prompts)\n",
    "                    z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device)) # [B, channel, c, h]\n",
    "                    # decode it\n",
    "                    samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
    "                                                unconditional_conditioning=uc,)\n",
    "\n",
    "                    x_samples = model.decode_first_stage(samples)\n",
    "                    print(x_samples.shape)\n",
    "                    for x_sample in x_samples:\n",
    "                        spec = x_sample[0].cpu().numpy()\n",
    "                        spec_ori = init_image[0][0].cpu().numpy()\n",
    "                        print(x_sample.shape,spec.shape,init_image.shape)\n",
    "                        wav = vocoder.vocode(spec)\n",
    "                        wav_ori = vocoder.vocode(spec_ori)\n",
    "                        soundfile.write(os.path.join(outpath, f'{prompt.replace(\" \", \"-\")}.wav'), wav, SAMPLE_RATE, 'FLOAT')\n",
    "                        soundfile.write(os.path.join(outpath, f'{prompt.replace(\" \", \"-\")}_ori.wav'), wav_ori, SAMPLE_RATE, 'FLOAT')\n",
    "                        base_count += 1\n",
    "                    all_samples.append(x_samples)\n",
    "\n",
    "\n",
    "    print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
    "            f\" \\nEnjoy.\")\n"
   ],
   "execution_count":20,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "\/data\/notebook_files\/Make-An-Audio-main\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"ddRhJPVZOXqDAfXgQbj8bL",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## EDIT THE CODE CELL BELOW\n",
    "\n",
    "This will represent our automated prompt-tuning with Pengi and Make-an-Audio"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"IFkbE1AEbkSvnu7kwD3pJn",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import sys\n",
    "sys.path.insert(0, \"\/data\/notebook_files\/Pengi\")"
   ],
   "execution_count":7,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"snrGVjqkdsehv0IEbmraIK",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "print(sys.path)"
   ],
   "execution_count":8,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "['\/data\/notebook_files\/Pengi', '\/data\/notebook_files\/Pengi', '\/data\/notebook_files', '\/opt\/datalore\/python', '\/var\/datalore\/manager\/.pip', '\/data\/workspace_files', '\/opt\/python\/lib\/python38.zip', '\/opt\/python\/lib\/python3.8', '\/opt\/python\/lib\/python3.8\/lib-dynload', '', '\/opt\/python\/envs\/minimal\/lib\/python3.8\/site-packages', '\/opt\/python\/envs\/minimal\/lib\/python3.8\/site-packages\/IPython\/extensions', '\/home\/datalore\/.ipython', '']\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"8Esq4SkEODOJOPArYhKvPI",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import os\n",
    "from Pengi.wrapper import PengiWrapper as Pengi\n",
    "\n",
    "opt = Opt()\n",
    "seed_everything(opt.seed)\n",
    "\n",
    "config = OmegaConf.load(f\"{opt.config}\")\n",
    "model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "hparams = {\n",
    "    'audio_sample_rate': SAMPLE_RATE,\n",
    "    'audio_num_mel_bins':80,\n",
    "    'fft_size': 1024,\n",
    "    'win_size': 1024,\n",
    "    'hop_size': 256,\n",
    "    'fmin': 0,\n",
    "    'fmax': 8000,\n",
    "    'batch_max_length': 1248, \n",
    "    'mode': 'pad', # pad,none,\n",
    "}\n",
    "melnet = MelNet(hparams)\n",
    "sampler = DDIMSampler(model)\n",
    "vocoder = VocoderBigVGAN(opt.vocoder_ckpt,device)\n",
    "\n",
    "os.makedirs(opt.outdir, exist_ok=True)\n",
    "outpath = opt.outdir\n",
    "\n",
    "batch_size = opt.n_samples # 一个prompt产生n_samples个结果\n",
    "if not opt.from_file: # load prompts from this file\n",
    "    prompt = opt.prompt\n",
    "    assert prompt is not None\n",
    "    data = [batch_size * [prompt]]\n",
    "else:\n",
    "    print(f\"reading prompts from {opt.from_file}\")\n",
    "    with open(opt.from_file, \"r\") as f:\n",
    "        data = f.read().splitlines()\n",
    "        data = list(chunk(data, batch_size))\n",
    "\n",
    "\n",
    "sample_path = os.path.join(outpath, \"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "\n",
    "assert os.path.isfile(opt.init_audio)\n",
    "init_image = load_audio(opt.init_audio,transform=melnet).to(device)\n",
    "init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
    "sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
    "\n",
    "assert 0. <= opt.strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "t_enc = int(opt.strength * opt.ddim_steps)\n",
    "print(f\"target t_enc is {t_enc} steps\")\n",
    "\n",
    "pengi_output_prompts = {prompt[0]: [] for prompt in data}\n",
    "\n",
    "with torch.no_grad():\n",
    "    with model.ema_scope():\n",
    "        tic = time.time()\n",
    "        all_samples = list()\n",
    "        old_encoded = None\n",
    "        last_pengi_output = None\n",
    "\n",
    "        for prompts in tqdm(data, desc=\"data\"): # goal prompt\n",
    "            uc = None\n",
    "            \n",
    "            if opt.scale != 1.0: # default=5.0\n",
    "                uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "            \n",
    "            for pengi_i in range(opt.pengi_iterations):\n",
    "                if old_encoded is None:\n",
    "                    c = model.get_learned_conditioning(prompts)\n",
    "                else:\n",
    "                    pengi_encoded = model.get_learned_conditioning(last_pengi_output)\n",
    "                    c = old_encoded + (old_encoded - pengi_encoded)\n",
    "                \n",
    "\n",
    "                old_encoded = c\n",
    "                    \n",
    "                # new_encoded_prompt = old_encoded + (old_encoded - result_encoded)\n",
    "\n",
    "                z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device)) # [B, channel, c, h]\n",
    "                # decode it\n",
    "                samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
    "                                            unconditional_conditioning=uc,)\n",
    "\n",
    "                x_samples = model.decode_first_stage(samples)\n",
    "                print(x_samples.shape)\n",
    "                for x_sample in x_samples:\n",
    "                    spec = x_sample[0].cpu().numpy()\n",
    "                    spec_ori = init_image[0][0].cpu().numpy()\n",
    "                    print(x_sample.shape,spec.shape,init_image.shape)\n",
    "                    wav = vocoder.vocode(spec)\n",
    "                    wav_ori = vocoder.vocode(spec_ori)\n",
    "                    soundfile.write(os.path.join(outpath, f'{prompts[0]}-{pengi_i}.wav'), wav, SAMPLE_RATE, 'FLOAT')\n",
    "                    soundfile.write(os.path.join(outpath, f'{prompts[0]}-{pengi_i}_ori.wav'), wav_ori, SAMPLE_RATE, 'FLOAT')\n",
    "                    base_count += 1\n",
    "\n",
    "                all_samples.append(x_samples)\n",
    "                pengi = Pengi(config=\"base\")\n",
    "\n",
    "                generated_responses = pengi.generate(audio_paths=[os.path.join(outpath, f'{prompts[0]}-{pengi_i}.wav')],\n",
    "                                                text_prompts=[\"generate audio caption \"],\n",
    "                                                add_texts=[\",\"],\n",
    "                                                max_len=30,\n",
    "                                                beam_size=6,\n",
    "                                                temperature=1.0,\n",
    "                                                stop_token=' <|endoftext|>')\n",
    "                \n",
    "                last_pengi_output = generated_responses[0][0][0] # this is the pengi prompts, taking the first of the 6 elements.\n",
    "                pengi_output_prompts[prompts[0]].append(last_pengi_output)\n",
    "\n",
    "print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
    "        f\" \\nEnjoy.\")\n",
    "print(f\"{pengi_output_prompts}\")"
   ],
   "execution_count":25,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Loading model from \/data\/notebook_files\/Make-An-Audio-main\/useful_ckpts\/maa1_full.ckpt\n",
      "Global Step: 3722264\n",
      "LatentDiffusion_audio: Running in eps-prediction mode\n",
      "DiffusionWrapper has 160.22 M params.\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 78, 78) = 24336 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "TextEncoder comes with 111.32 M params.\n",
      "target t_enc is 30 steps\n",
      "torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "Your samples are ready and waiting for you here: \n",
      "\/data\/notebook_files\/audio-outputs-with-pengi \n",
      " \n",
      "Enjoy.\n",
      "{'a warm melodic sounding violin': [' a violin is playing on a violin. ', ' a violin riff is being played on a violin. ', ' a person is playing a guitar. ', ' while a machine is running, a person is making a sound. ', ' a machine is being used to make a sound. ']}\n"
     ],
     "output_type":"stream"
    },
    {
     "name":"stderr",
     "text":[
      "Global seed set to 42\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\rdata:   0%|          | 0\/1 [00:00<?, ?it\/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\/opt\/python\/envs\/minimal\/lib\/python3.8\/site-packages\/transformers\/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\rdata: 100%|██████████| 1\/1 [04:17<00:00, 257.11s\/it]\rdata: 100%|██████████| 1\/1 [04:17<00:00, 257.12s\/it]\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"4rYMCipUAMGy8T3jfgLqTK",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "prompt"
   ],
   "execution_count":23,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "'a warm melodic sounding violin'"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"z2KCpbXZmvxhkITUcHQXyZ",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "[x[0] for x in data]"
   ],
   "execution_count":16,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "['a bird chirping']"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"hNMoePoQFGMz4FsfpNduJD",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "c.shape"
   ],
   "execution_count":30,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "torch.Size([2, 77, 1024])"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"ymPtfydnATnE2SjU0sNUIG",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "z_enc.shape"
   ],
   "execution_count":32,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "torch.Size([2, 4, 10, 78])"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"33pvOf39mCUPmaEsGc207R",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "It seems like we're typically losing *adjectives*"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"yadwM8gqO1iDfhEZPHUScA",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Sheet 2"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"Sheet 2",
     "type":"MD",
     "hide_input_from_viewers":false,
     "hide_output_from_viewers":false,
     "sheet_delimiter":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "# Pengi Prompt Tuning on Make-an-Audio\n",
    "## Add all the necessary packages to PATH"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"N4fE28fhAlR1t4IvsdDFXi",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "import sys\n",
    "sys.path.insert(0, \"\/data\/notebook_files\/Pengi\")\n",
    "sys.path.insert(0, \"\/data\/notebook_files\/Make-An-Audio-main\")"
   ],
   "execution_count":1,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"ifjBpuP4bUaJLp5gGORKIC",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Import all the modules that are necessary for the experiment to work"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"ju0J6atv2u6zv9X9jhd1gd",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\"\"\"make variations of input image\"\"\"\n",
    "import argparse, os, glob\n",
    "import PIL\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange, repeat\n",
    "from torchvision.utils import make_grid\n",
    "from torch import autocast\n",
    "import librosa\n",
    "# from contextlib import nullcontext\n",
    "import time\n",
    "from pytorch_lightning import seed_everything\n",
    "import math\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from vocoder.bigvgan.models import VocoderBigVGAN\n",
    "# from ldm.data.extract_mel_spectrogram import TRANSFORMS_22050,TRANSFORMS_16000\n",
    "from preprocess.NAT_mel import MelNet\n",
    "import soundfile\n",
    "\n",
    "batch_max_length = 624\n",
    "SAMPLE_RATE= 16000"
   ],
   "execution_count":2,
   "outputs":[
    {
     "name":"stderr",
     "text":[
      "\/opt\/python\/envs\/minimal\/lib\/python3.8\/site-packages\/tqdm\/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https:\/\/ipywidgets.readthedocs.io\/en\/stable\/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"O8xB07cheBoRV99CmCSFu9",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Create helper functions"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"Xj01Z1RdZAAwti8qpIxOH4",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=True):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_audio(path,transform,sr=16000,batch_max_length=624):# load wav and return mel\n",
    "    wav,_ = librosa.load(path,sr=sr)\n",
    "\n",
    "    audio = transform(wav) # (1,melbins,T)\n",
    "    if audio.shape[2] <= batch_max_length:\n",
    "        n_repeat = math.ceil((batch_max_length + 1) \/ audio.shape[1])\n",
    "        audio = audio.repeat(1,1, n_repeat)\n",
    "\n",
    "    audio = audio[..., :batch_max_length].unsqueeze(0) # shape [1,1,80,batch_max_length]\n",
    "    return audio\n",
    "\n",
    "\n",
    "def load_img(path):# load mel\n",
    "    audio = np.load(path)\n",
    "    if audio.shape[1] <= batch_max_length:\n",
    "        n_repeat = math.ceil((batch_max_length + 1) \/ audio.shape[1])\n",
    "        audio = np.tile(audio, reps=(1, n_repeat))\n",
    "\n",
    "    audio = audio[:, :batch_max_length]\n",
    "    audio = torch.FloatTensor(audio)[None, None, :, :] # [1,1,80,batch_max_length]\n",
    "    return audio\n",
    "\n",
    "\n",
    "class Opt:\n",
    "    def __init__(self):\n",
    "        self.config = \"\/data\/notebook_files\/Make-An-Audio-main\/configs\/text_to_audio\/txt2audio_args.yaml\"\n",
    "        self.ckpt = \"\/data\/notebook_files\/Make-An-Audio-main\/useful_ckpts\/maa1_full.ckpt\"\n",
    "        self.vocoder_ckpt = \"\/data\/notebook_files\/Make-An-Audio-main\/useful_ckpts\/bigvnat\"\n",
    "        self.outdir = \"\/data\/notebook_files\/audio-outputs-with-pengi\"\n",
    "        self.from_file = None\n",
    "        \n",
    "        self.strength = 0.3\n",
    "        self.seed = 42\n",
    "        self.scale = 3.0\n",
    "        self.n_samples = 2\n",
    "        self.n_iter = 1\n",
    "\n",
    "        self.ddim_steps = 100\n",
    "        self.ddim_eta = 0.0\n",
    "\n",
    "        self.prompt = \"a warm melodic sounding violin\"\n",
    "        self.init_audio = \"\/data\/notebook_files\/violin_0.wav\"\n",
    "        self.pengi_iterations = 5"
   ],
   "execution_count":3,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"RuzOwt3aZXQ3rYxDPAoFxE",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Run experiment 1\n",
    "In this experiment, we will be iterating on the sound by modifying the prompt embedding by trying to empasize the parts that were missed in the previous run."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"mTaokE16zOM0cuTyAAn6P9",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from Pengi.wrapper import PengiWrapper as Pengi\n",
    "\n",
    "opt = Opt()\n",
    "seed_everything(opt.seed)\n",
    "\n",
    "config = OmegaConf.load(f\"{opt.config}\")\n",
    "model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "hparams = {\n",
    "    'audio_sample_rate': SAMPLE_RATE,\n",
    "    'audio_num_mel_bins':80,\n",
    "    'fft_size': 1024,\n",
    "    'win_size': 1024,\n",
    "    'hop_size': 256,\n",
    "    'fmin': 0,\n",
    "    'fmax': 8000,\n",
    "    'batch_max_length': 1248, \n",
    "    'mode': 'pad', # pad,none,\n",
    "}\n",
    "\n",
    "melnet = MelNet(hparams)\n",
    "sampler = DDIMSampler(model)\n",
    "vocoder = VocoderBigVGAN(opt.vocoder_ckpt,device)\n",
    "\n",
    "os.makedirs(opt.outdir, exist_ok=True)\n",
    "outpath = opt.outdir\n",
    "\n",
    "batch_size = opt.n_samples # 一个prompt产生n_samples个结果\n",
    "if not opt.from_file: # load prompts from this file\n",
    "    prompt = opt.prompt\n",
    "    assert prompt is not None\n",
    "    data = [batch_size * [prompt]]\n",
    "else:\n",
    "    print(f\"reading prompts from {opt.from_file}\")\n",
    "    with open(opt.from_file, \"r\") as f:\n",
    "        data = f.read().splitlines()\n",
    "        data = list(chunk(data, batch_size))\n",
    "\n",
    "\n",
    "sample_path = os.path.join(outpath, \"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "\n",
    "\n",
    "assert os.path.isfile(opt.init_audio)\n",
    "init_image = load_audio(opt.init_audio,transform=melnet).to(device)\n",
    "init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
    "sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
    "\n",
    "\n",
    "assert 0. <= opt.strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "t_enc = int(opt.strength * opt.ddim_steps)\n",
    "print(f\"target t_enc is {t_enc} steps\")\n",
    "\n",
    "\n",
    "pengi_output_prompts = {prompt[0]: [] for prompt in data}\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    with model.ema_scope():\n",
    "        tic = time.time()\n",
    "        all_samples = list()\n",
    "        old_encoded = None\n",
    "        last_pengi_output = None\n",
    "\n",
    "        for prompts in tqdm(data, desc=\"data\"): # goal prompt\n",
    "            uc = None\n",
    "            \n",
    "            if opt.scale != 1.0: # default=5.0\n",
    "                uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "            \n",
    "            for pengi_i in range(opt.pengi_iterations):\n",
    "                if old_encoded is None:\n",
    "                    c = model.get_learned_conditioning(prompts)\n",
    "                else:\n",
    "                    pengi_encoded = model.get_learned_conditioning(last_pengi_output)\n",
    "                    c = old_encoded + (old_encoded - pengi_encoded)\n",
    "                \n",
    "                old_encoded = c\n",
    "\n",
    "                z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device)) # [B, channel, c, h]\n",
    "                # decode it\n",
    "                samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
    "                                            unconditional_conditioning=uc,)\n",
    "\n",
    "                x_samples = model.decode_first_stage(samples)\n",
    "                print(x_samples.shape)\n",
    "                for x_sample in x_samples:\n",
    "                    spec = x_sample[0].cpu().numpy()\n",
    "                    spec_ori = init_image[0][0].cpu().numpy()\n",
    "                    print(x_sample.shape,spec.shape,init_image.shape)\n",
    "                    wav = vocoder.vocode(spec)\n",
    "                    wav_ori = vocoder.vocode(spec_ori)\n",
    "                    soundfile.write(os.path.join(outpath, f'{prompts[0]}-{pengi_i}.wav'), wav, SAMPLE_RATE, 'FLOAT')\n",
    "                    soundfile.write(os.path.join(outpath, f'{prompts[0]}-{pengi_i}_ori.wav'), wav_ori, SAMPLE_RATE, 'FLOAT')\n",
    "                    base_count += 1\n",
    "\n",
    "                all_samples.append(x_samples)\n",
    "                pengi = Pengi(config=\"base\")\n",
    "\n",
    "                generated_responses = pengi.generate(audio_paths=[os.path.join(outpath, f'{prompts[0]}-{pengi_i}.wav')],\n",
    "                                                text_prompts=[\"generate audio caption \"],\n",
    "                                                add_texts=[\",\"],\n",
    "                                                max_len=30,\n",
    "                                                beam_size=6,\n",
    "                                                temperature=1.0,\n",
    "                                                stop_token=' <|endoftext|>')\n",
    "                \n",
    "                last_pengi_output = generated_responses[0][0][0] # this is the pengi prompts, taking the first of the 6 elements.\n",
    "                pengi_output_prompts[prompts[0]].append(last_pengi_output)\n",
    "\n",
    "print(f\"{pengi_output_prompts}\")"
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"NiFuRE00saytqASPcA22CV",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Conclusion\n",
    "It appears we are diverging from the goal after the second iteration."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"Du0y6q2hMhoUzerh49Z3mV",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Experiment 2\n",
    "Try to interpolate the prompts"
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"hXiH4pPLXOxnW7YaOQy9KF",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from Pengi.wrapper import PengiWrapper as Pengi\n",
    "\n",
    "opt = Opt()\n",
    "opt.init_audio = \"\/data\/notebook_files\/source_unmodified_audios\/harp.wav\"\n",
    "opt.prompt = \"A beautiful, melodic sounding harp\"\n",
    "opt.outdir = \"\/data\/notebook_files\/audio-outputs-with-pengi\/experiment2\"\n",
    "seed_everything(opt.seed)\n",
    "\n",
    "config = OmegaConf.load(f\"{opt.config}\")\n",
    "model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "hparams = {\n",
    "    'audio_sample_rate': SAMPLE_RATE,\n",
    "    'audio_num_mel_bins':80,\n",
    "    'fft_size': 1024,\n",
    "    'win_size': 1024,\n",
    "    'hop_size': 256,\n",
    "    'fmin': 0,\n",
    "    'fmax': 8000,\n",
    "    'batch_max_length': 1248, \n",
    "    'mode': 'pad', # pad,none,\n",
    "}\n",
    "\n",
    "melnet = MelNet(hparams)\n",
    "sampler = DDIMSampler(model)\n",
    "vocoder = VocoderBigVGAN(opt.vocoder_ckpt,device)\n",
    "\n",
    "os.makedirs(opt.outdir, exist_ok=True)\n",
    "outpath = opt.outdir\n",
    "\n",
    "batch_size = opt.n_samples # 一个prompt产生n_samples个结果\n",
    "if not opt.from_file: # load prompts from this file\n",
    "    prompt = opt.prompt\n",
    "    assert prompt is not None\n",
    "    data = [batch_size * [prompt]]\n",
    "else:\n",
    "    print(f\"reading prompts from {opt.from_file}\")\n",
    "    with open(opt.from_file, \"r\") as f:\n",
    "        data = f.read().splitlines()\n",
    "        data = list(chunk(data, batch_size))\n",
    "\n",
    "\n",
    "sample_path = os.path.join(outpath, \"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "\n",
    "\n",
    "assert os.path.isfile(opt.init_audio)\n",
    "init_image = load_audio(opt.init_audio,transform=melnet).to(device)\n",
    "init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
    "sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
    "\n",
    "\n",
    "assert 0. <= opt.strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "t_enc = int(opt.strength * opt.ddim_steps)\n",
    "print(f\"target t_enc is {t_enc} steps\")\n",
    "\n",
    "\n",
    "pengi_output_prompts = {prompt[0]: [] for prompt in data}\n",
    "a = 0.5\n",
    "\n",
    "with torch.no_grad():\n",
    "    with model.ema_scope():\n",
    "        tic = time.time()\n",
    "        all_samples = list()\n",
    "        old_encoded = None\n",
    "        last_pengi_output = None\n",
    "\n",
    "        for prompts in tqdm(data, desc=\"data\"): # goal prompt\n",
    "            uc = None\n",
    "            \n",
    "            if opt.scale != 1.0: # default=5.0\n",
    "                uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "            \n",
    "            for pengi_i in range(opt.pengi_iterations):\n",
    "                if old_encoded is None:\n",
    "                    c = model.get_learned_conditioning(prompts)\n",
    "                else:\n",
    "                    pengi_encoded = model.get_learned_conditioning(last_pengi_output)\n",
    "                    c = (a * pengi_encoded) + ((1 - a) * old_encoded)\n",
    "                \n",
    "                old_encoded = c\n",
    "\n",
    "                z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device)) # [B, channel, c, h]\n",
    "                # decode it\n",
    "                samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
    "                                            unconditional_conditioning=uc,)\n",
    "\n",
    "                x_samples = model.decode_first_stage(samples)\n",
    "                print(x_samples.shape)\n",
    "                for x_sample in x_samples:\n",
    "                    spec = x_sample[0].cpu().numpy()\n",
    "                    spec_ori = init_image[0][0].cpu().numpy()\n",
    "                    print(x_sample.shape,spec.shape,init_image.shape)\n",
    "                    wav = vocoder.vocode(spec)\n",
    "                    wav_ori = vocoder.vocode(spec_ori)\n",
    "                    soundfile.write(os.path.join(outpath, f'{prompts[0]}-{pengi_i}.wav'), wav, SAMPLE_RATE, 'FLOAT')\n",
    "                    soundfile.write(os.path.join(outpath, f'{prompts[0]}-{pengi_i}_ori.wav'), wav_ori, SAMPLE_RATE, 'FLOAT')\n",
    "                    base_count += 1\n",
    "\n",
    "                all_samples.append(x_samples)\n",
    "                pengi = Pengi(config=\"base\")\n",
    "\n",
    "                generated_responses = pengi.generate(audio_paths=[os.path.join(outpath, f'{prompts[0]}-{pengi_i}.wav')],\n",
    "                                                text_prompts=[\"generate audio caption \"],\n",
    "                                                add_texts=[\",\"],\n",
    "                                                max_len=30,\n",
    "                                                beam_size=6,\n",
    "                                                temperature=1.0,\n",
    "                                                stop_token=' <|endoftext|>')\n",
    "                \n",
    "                last_pengi_output = generated_responses[0][0][0] # this is the pengi prompts, taking the first of the 6 elements.\n",
    "                pengi_output_prompts[prompts[0]].append(last_pengi_output)\n",
    "\n",
    "print(f\"{pengi_output_prompts}\")"
   ],
   "execution_count":13,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Loading model from \/data\/notebook_files\/Make-An-Audio-main\/useful_ckpts\/maa1_full.ckpt\n",
      "Global Step: 3722264\n",
      "LatentDiffusion_audio: Running in eps-prediction mode\n",
      "DiffusionWrapper has 160.22 M params.\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 78, 78) = 24336 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "TextEncoder comes with 111.32 M params.\n",
      "target t_enc is 30 steps\n",
      "torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "{'A beautiful, melodic sounding harp': [' a guitar is being played. ', ' a guitar is being played. ', ' a guitar is being played. ', ' a guitar is being played. ', ' a guitar is being played. ']}\n"
     ],
     "output_type":"stream"
    },
    {
     "name":"stderr",
     "text":[
      "Global seed set to 42\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\rdata:   0%|          | 0\/1 [00:00<?, ?it\/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\rdata: 100%|██████████| 1\/1 [03:58<00:00, 238.68s\/it]\rdata: 100%|██████████| 1\/1 [03:58<00:00, 238.68s\/it]\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"QTyfmndqYsd2zBde4THmYQ",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "### Observations\n",
    "It seems like we're losing *adjectival* language in our Pengi outputs. \n",
    "\n",
    "Could we coerce it to use more adjectives?\n",
    "If we can accomplish this, will it make an useful difference in our outputs?\n",
    "\n",
    "\n",
    "\n",
    "We're also currently doing top 1. What if we weighted in a top-K sense (using Pengi's generative results, rescaled to a pdf) and then used a tree-type structure with pruning later to branch results (and possibly pass original inputs through as well)? "
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"LWZSkLycLArjF7jiJEoHCU",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Idea for experiment 3:\n",
    "original_prompt: \"Make the violin sound warmer\"\n",
    "\n",
    "description: \"Right now this audio is ...\"\n",
    "\n",
    "description + original_prompt -> bert\n",
    "\n",
    "feed the new audio with the new bert embedding."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"gGWpVz1vd6VXpQ3XNzHXKe",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from Pengi.wrapper import PengiWrapper as Pengi\n",
    "\n",
    "opt = Opt()\n",
    "opt.outdir = \"\/data\/notebook_files\/audio-outputs-with-pengi\/experiment3\"\n",
    "seed_everything(opt.seed)\n",
    "\n",
    "config = OmegaConf.load(f\"{opt.config}\")\n",
    "model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "hparams = {\n",
    "    'audio_sample_rate': SAMPLE_RATE,\n",
    "    'audio_num_mel_bins':80,\n",
    "    'fft_size': 1024,\n",
    "    'win_size': 1024,\n",
    "    'hop_size': 256,\n",
    "    'fmin': 0,\n",
    "    'fmax': 8000,\n",
    "    'batch_max_length': 1248, \n",
    "    'mode': 'pad', # pad,none,\n",
    "}\n",
    "\n",
    "melnet = MelNet(hparams)\n",
    "sampler = DDIMSampler(model)\n",
    "vocoder = VocoderBigVGAN(opt.vocoder_ckpt,device)\n",
    "\n",
    "os.makedirs(opt.outdir, exist_ok=True)\n",
    "outpath = opt.outdir\n",
    "\n",
    "batch_size = opt.n_samples # 一个prompt产生n_samples个结果\n",
    "if not opt.from_file: # load prompts from this file\n",
    "    prompt = opt.prompt\n",
    "    assert prompt is not None\n",
    "    data = [batch_size * [prompt]]\n",
    "else:\n",
    "    print(f\"reading prompts from {opt.from_file}\")\n",
    "    with open(opt.from_file, \"r\") as f:\n",
    "        data = f.read().splitlines()\n",
    "        data = list(chunk(data, batch_size))\n",
    "\n",
    "\n",
    "sample_path = os.path.join(outpath, \"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "\n",
    "\n",
    "assert os.path.isfile(opt.init_audio)\n",
    "init_image = load_audio(opt.init_audio,transform=melnet).to(device)\n",
    "init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
    "sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
    "\n",
    "\n",
    "assert 0. <= opt.strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "t_enc = int(opt.strength * opt.ddim_steps)\n",
    "print(f\"target t_enc is {t_enc} steps\")\n",
    "\n",
    "\n",
    "pengi_output_prompts = {prompt[0]: [] for prompt in data}\n",
    "a = 0.5\n",
    "\n",
    "with torch.no_grad():\n",
    "    with model.ema_scope():\n",
    "        tic = time.time()\n",
    "        all_samples = list()\n",
    "        old_encoded = None\n",
    "        last_pengi_output = None\n",
    "\n",
    "        for prompts in tqdm(data, desc=\"data\"): # goal prompt\n",
    "            uc = None\n",
    "            \n",
    "            if opt.scale != 1.0: # default=5.0\n",
    "                uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "            \n",
    "            for pengi_i in range(opt.pengi_iterations):\n",
    "                if old_encoded is None:\n",
    "                    c = model.get_learned_conditioning(prompts)\n",
    "                else:\n",
    "                    pengi_encoded = model.get_learned_conditioning(last_pengi_output)\n",
    "                    c = (a * pengi_encoded) + ((1 - a) * old_encoded)\n",
    "                \n",
    "                old_encoded = c\n",
    "\n",
    "                z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device)) # [B, channel, c, h]\n",
    "                # decode it\n",
    "                samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
    "                                            unconditional_conditioning=uc,)\n",
    "\n",
    "                x_samples = model.decode_first_stage(samples)\n",
    "                print(x_samples.shape)\n",
    "                for x_sample in x_samples:\n",
    "                    spec = x_sample[0].cpu().numpy()\n",
    "                    spec_ori = init_image[0][0].cpu().numpy()\n",
    "                    print(x_sample.shape,spec.shape,init_image.shape)\n",
    "                    wav = vocoder.vocode(spec)\n",
    "                    wav_ori = vocoder.vocode(spec_ori)\n",
    "                    soundfile.write(os.path.join(outpath, f'{prompts[0]}-{pengi_i}.wav'), wav, SAMPLE_RATE, 'FLOAT')\n",
    "                    soundfile.write(os.path.join(outpath, f'{prompts[0]}-{pengi_i}_ori.wav'), wav_ori, SAMPLE_RATE, 'FLOAT')\n",
    "                    base_count += 1\n",
    "\n",
    "                all_samples.append(x_samples)\n",
    "                pengi = Pengi(config=\"base\")\n",
    "                \n",
    "                beam_size = 8\n",
    "                random_prompt_index = np.random.randint(0, beam_size)\n",
    "                generated_responses = pengi.generate(audio_paths=[os.path.join(outpath, f'{prompts[0]}-{pengi_i}.wav')],\n",
    "                                                text_prompts=[\"generate an audio caption with stylistic adjectives \"],\n",
    "                                                add_texts=[\",\"],\n",
    "                                                max_len=30,\n",
    "                                                beam_size=beam_size,\n",
    "                                                temperature=2,\n",
    "                                                stop_token=' <|endoftext|>')\n",
    "                \n",
    "                last_pengi_output = generated_responses[0][0][random_prompt_index] # this is the pengi prompts, taking the first of the 6 elements.\n",
    "                pengi_output_prompts[prompts[0]].append(last_pengi_output)\n",
    "\n",
    "print(f\"{pengi_output_prompts}\")"
   ],
   "execution_count":19,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Loading model from \/data\/notebook_files\/Make-An-Audio-main\/useful_ckpts\/maa1_full.ckpt\n",
      "Global Step: 3722264\n",
      "LatentDiffusion_audio: Running in eps-prediction mode\n",
      "DiffusionWrapper has 160.22 M params.\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 78, 78) = 24336 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "TextEncoder comes with 111.32 M params.\n",
      "target t_enc is 30 steps\n",
      "torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "{'a warm melodic sounding violin': [' violin riff ', ' viola riff ', ' violin riff ', ' viola lick ', ' viola playing on a violin ']}\n"
     ],
     "output_type":"stream"
    },
    {
     "name":"stderr",
     "text":[
      "Global seed set to 42\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\rdata:   0%|          | 0\/1 [00:00<?, ?it\/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\/opt\/python\/envs\/minimal\/lib\/python3.8\/site-packages\/transformers\/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\rdata: 100%|██████████| 1\/1 [04:15<00:00, 255.96s\/it]\rdata: 100%|██████████| 1\/1 [04:15<00:00, 255.97s\/it]\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"pTTed9KwPGyUCFMpvEytrS",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"markdown",
   "source":[
    "## Idea for experiment 4:\n",
    "\n",
    "original prompt: \"Make the violin sound warmer warmer warmer warmer\"\n",
    "\n",
    "description: \"Right now this audio is ...\"\n",
    "\n",
    "description + original_prompt -> bert\n",
    "\n",
    "feed the new audio with the new bert embedding."
   ],
   "attachments":{
    
   },
   "metadata":{
    "datalore":{
     "node_id":"lv51vgcK6fnH1kXBDv46qV",
     "type":"MD",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    
   ],
   "execution_count":null,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"tad8Tae4MpPdt0ngoL64a8",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"minimal",
   "packages":[
    {
     "name":"librosa",
     "version":"0.10.0.post2",
     "source":"PIP"
    },
    {
     "name":"PyYAML",
     "version":"6.0",
     "source":"PIP"
    },
    {
     "name":"PyYAML",
     "version":"6.0",
     "source":"PIP"
    },
    {
     "name":"torch",
     "version":"2.0.1",
     "source":"PIP"
    },
    {
     "name":"torch-fidelity",
     "version":"0.3.0",
     "source":"PIP"
    },
    {
     "name":"scipy",
     "source":"PIP"
    },
    {
     "name":"importlib_resources",
     "version":"5.12.0",
     "source":"PIP"
    },
    {
     "name":"torchaudio>=0.13.0",
     "source":"PIP"
    },
    {
     "name":"torchvision>=0.14.0",
     "source":"PIP"
    },
    {
     "name":"tqdm",
     "source":"PIP"
    },
    {
     "name":"omegaconf",
     "source":"PIP"
    },
    {
     "name":"einops",
     "source":"PIP"
    },
    {
     "name":"numpy<=1.23.5",
     "source":"PIP"
    },
    {
     "name":"soundfile",
     "source":"PIP"
    },
    {
     "name":"pandas",
     "source":"PIP"
    },
    {
     "name":"torchlibrosa ",
     "version":" 0.1.0",
     "source":"PIP"
    },
    {
     "name":"transformers",
     "version":"4.18.0",
     "source":"PIP"
    },
    {
     "name":"ftfy",
     "source":"PIP"
    },
    {
     "name":"pytorch-lightning",
     "version":"1.7.0",
     "source":"PIP"
    },
    {
     "name":"torchmetrics",
     "version":"0.11.1",
     "source":"PIP"
    },
    {
     "name":"sentence-transformers",
     "source":"PIP"
    },
    {
     "name":"git+https:\/\/github.com\/CompVis\/taming-transformers.git@master#egg=taming-transformers",
     "source":"PIP"
    }
   ],
   "report_row_ids":[
    
   ],
   "version":3
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}