{
 "cells":[
  {
   "cell_type":"code",
   "source":[
    "import os\n",
    "\n",
    "os.chdir(\"\/data\/notebook_files\/Pengi\")"
   ],
   "execution_count":1,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"CEmny4RGeRi3gCRQ5HhaHK",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from Pengi.wrapper import PengiWrapper as Pengi\n",
    "\n",
    "pengi = Pengi(config=\"base\")\n",
    "\n",
    "generated_responses = pengi.generate(audio_paths=[\"\/data\/notebook_files\/violin_0.wav\"],\n",
    "                                   text_prompts=[\"generate audio caption \"],\n",
    "                                   add_texts=[\",\"],\n",
    "                                   max_len=30,\n",
    "                                   beam_size=6,\n",
    "                                   temperature=1.0,\n",
    "                                   stop_token=' <|endoftext|>'\n",
    "                                   )"
   ],
   "execution_count":2,
   "outputs":[
    {
     "name":"stderr",
     "text":[
      "\/opt\/python\/envs\/minimal\/lib\/python3.8\/site-packages\/tqdm\/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https:\/\/ipywidgets.readthedocs.io\/en\/stable\/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\/opt\/python\/envs\/minimal\/lib\/python3.8\/site-packages\/torch\/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ..\/aten\/src\/ATen\/native\/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "\rDownloading:   0%|          | 0.00\/4.01k [00:00<?, ?B\/s]\rDownloading: 100%|██████████| 4.01k\/4.01k [00:00<00:00, 11.5MB\/s]\n",
      "\rDownloading:   0%|          | 0.00\/571M [00:00<?, ?B\/s]\rDownloading:   1%|          | 4.92M\/571M [00:00<00:11, 51.5MB\/s]\rDownloading:   2%|▏         | 9.84M\/571M [00:00<00:11, 51.6MB\/s]\rDownloading:   3%|▎         | 14.9M\/571M [00:00<00:11, 52.3MB\/s]\rDownloading:   3%|▎         | 19.9M\/571M [00:00<00:12, 46.7MB\/s]\rDownloading:   4%|▍         | 25.2M\/571M [00:00<00:11, 49.6MB\/s]\rDownloading:   5%|▌         | 30.3M\/571M [00:00<00:11, 51.1MB\/s]\rDownloading:   6%|▌         | 35.5M\/571M [00:00<00:10, 52.1MB\/s]\rDownloading:   7%|▋         | 40.7M\/571M [00:00<00:10, 52.8MB\/s]\rDownloading:   8%|▊         | 45.7M\/571M [00:00<00:10, 51.9MB\/s]\rDownloading:   9%|▉         | 50.7M\/571M [00:01<00:10, 51.2MB\/s]\rDownloading:  10%|▉         | 55.6M\/571M [00:01<00:10, 50.5MB\/s]\rDownloading:  11%|█         | 61.0M\/571M [00:01<00:10, 52.3MB\/s]\rDownloading:  12%|█▏        | 66.1M\/571M [00:01<00:10, 52.6MB\/s]\rDownloading:  12%|█▏        | 71.3M\/571M [00:01<00:09, 53.2MB\/s]\rDownloading:  13%|█▎        | 76.4M\/571M [00:01<00:10, 51.0MB\/s]\rDownloading:  14%|█▍        | 81.8M\/571M [00:01<00:09, 52.6MB\/s]\rDownloading:  15%|█▌        | 87.0M\/571M [00:01<00:09, 53.0MB\/s]\rDownloading:  16%|█▌        | 92.0M\/571M [00:01<00:09, 52.3MB\/s]\rDownloading:  17%|█▋        | 97.0M\/571M [00:01<00:09, 52.1MB\/s]\rDownloading:  18%|█▊        | 102M\/571M [00:02<00:09, 51.2MB\/s] \rDownloading:  19%|█▊        | 107M\/571M [00:02<00:09, 51.6MB\/s]\rDownloading:  20%|█▉        | 112M\/571M [00:02<00:09, 53.3MB\/s]\rDownloading:  21%|██        | 118M\/571M [00:02<00:09, 51.8MB\/s]\rDownloading:  22%|██▏       | 123M\/571M [00:02<00:08, 52.7MB\/s]\rDownloading:  22%|██▏       | 128M\/571M [00:02<00:08, 52.5MB\/s]\rDownloading:  23%|██▎       | 133M\/571M [00:02<00:08, 51.9MB\/s]\rDownloading:  24%|██▍       | 138M\/571M [00:02<00:08, 51.5MB\/s]\rDownloading:  25%|██▌       | 143M\/571M [00:02<00:08, 53.1MB\/s]\rDownloading:  26%|██▌       | 148M\/571M [00:02<00:08, 52.2MB\/s]\rDownloading:  27%|██▋       | 153M\/571M [00:03<00:08, 50.7MB\/s]\rDownloading:  28%|██▊       | 158M\/571M [00:03<00:08, 50.2MB\/s]\rDownloading:  29%|██▊       | 163M\/571M [00:03<00:08, 50.5MB\/s]\rDownloading:  29%|██▉       | 168M\/571M [00:03<00:08, 52.0MB\/s]\rDownloading:  30%|███       | 173M\/571M [00:03<00:07, 52.5MB\/s]\rDownloading:  31%|███▏      | 178M\/571M [00:03<00:08, 51.4MB\/s]\rDownloading:  32%|███▏      | 183M\/571M [00:03<00:08, 50.8MB\/s]\rDownloading:  33%|███▎      | 188M\/571M [00:03<00:07, 51.5MB\/s]\rDownloading:  34%|███▍      | 193M\/571M [00:03<00:07, 51.6MB\/s]\rDownloading:  35%|███▍      | 199M\/571M [00:04<00:07, 52.8MB\/s]\rDownloading:  36%|███▌      | 204M\/571M [00:04<00:07, 52.3MB\/s]\rDownloading:  37%|███▋      | 209M\/571M [00:04<00:07, 51.9MB\/s]\rDownloading:  37%|███▋      | 214M\/571M [00:04<00:07, 50.9MB\/s]\rDownloading:  38%|███▊      | 219M\/571M [00:04<00:07, 51.9MB\/s]\rDownloading:  39%|███▉      | 224M\/571M [00:04<00:06, 53.1MB\/s]\rDownloading:  40%|████      | 229M\/571M [00:04<00:06, 51.4MB\/s]\rDownloading:  41%|████      | 234M\/571M [00:04<00:06, 52.0MB\/s]\rDownloading:  42%|████▏     | 240M\/571M [00:04<00:06, 53.3MB\/s]\rDownloading:  43%|████▎     | 245M\/571M [00:04<00:06, 52.7MB\/s]\rDownloading:  44%|████▍     | 250M\/571M [00:05<00:06, 51.9MB\/s]\rDownloading:  45%|████▍     | 255M\/571M [00:05<00:06, 52.3MB\/s]\rDownloading:  46%|████▌     | 260M\/571M [00:05<00:06, 53.5MB\/s]\rDownloading:  46%|████▋     | 265M\/571M [00:05<00:06, 52.0MB\/s]\rDownloading:  47%|████▋     | 270M\/571M [00:05<00:06, 52.0MB\/s]\rDownloading:  48%|████▊     | 276M\/571M [00:05<00:05, 53.3MB\/s]\rDownloading:  49%|████▉     | 281M\/571M [00:05<00:05, 52.9MB\/s]\rDownloading:  50%|█████     | 286M\/571M [00:05<00:05, 51.5MB\/s]\rDownloading:  51%|█████     | 291M\/571M [00:05<00:05, 53.0MB\/s]\rDownloading:  52%|█████▏    | 296M\/571M [00:05<00:05, 52.1MB\/s]\rDownloading:  53%|█████▎    | 302M\/571M [00:06<00:05, 52.7MB\/s]\rDownloading:  54%|█████▎    | 307M\/571M [00:06<00:05, 51.2MB\/s]\rDownloading:  55%|█████▍    | 312M\/571M [00:06<00:05, 53.0MB\/s]\rDownloading:  56%|█████▌    | 317M\/571M [00:06<00:05, 52.1MB\/s]\rDownloading:  56%|█████▋    | 322M\/571M [00:06<00:05, 51.2MB\/s]\rDownloading:  57%|█████▋    | 328M\/571M [00:06<00:04, 53.0MB\/s]\rDownloading:  58%|█████▊    | 333M\/571M [00:06<00:04, 52.3MB\/s]\rDownloading:  59%|█████▉    | 338M\/571M [00:06<00:04, 52.2MB\/s]\rDownloading:  60%|██████    | 343M\/571M [00:06<00:04, 52.4MB\/s]\rDownloading:  61%|██████    | 348M\/571M [00:07<00:04, 52.9MB\/s]\rDownloading:  62%|██████▏   | 353M\/571M [00:07<00:04, 53.0MB\/s]\rDownloading:  63%|██████▎   | 358M\/571M [00:07<00:04, 53.2MB\/s]\rDownloading:  64%|██████▎   | 363M\/571M [00:07<00:04, 53.5MB\/s]\rDownloading:  65%|██████▍   | 369M\/571M [00:07<00:03, 54.3MB\/s]\rDownloading:  65%|██████▌   | 374M\/571M [00:07<00:03, 54.3MB\/s]\rDownloading:  66%|██████▋   | 379M\/571M [00:07<00:03, 55.3MB\/s]\rDownloading:  67%|██████▋   | 385M\/571M [00:07<00:03, 56.1MB\/s]\rDownloading:  68%|██████▊   | 390M\/571M [00:07<00:03, 55.3MB\/s]\rDownloading:  69%|██████▉   | 396M\/571M [00:07<00:03, 56.2MB\/s]\rDownloading:  70%|███████   | 401M\/571M [00:08<00:03, 55.8MB\/s]\rDownloading:  71%|███████   | 407M\/571M [00:08<00:03, 55.2MB\/s]\rDownloading:  72%|███████▏  | 412M\/571M [00:08<00:03, 55.1MB\/s]\rDownloading:  73%|███████▎  | 417M\/571M [00:08<00:02, 54.8MB\/s]\rDownloading:  74%|███████▍  | 422M\/571M [00:08<00:02, 54.5MB\/s]\rDownloading:  75%|███████▍  | 427M\/571M [00:08<00:02, 53.6MB\/s]\rDownloading:  76%|███████▌  | 433M\/571M [00:08<00:02, 53.3MB\/s]\rDownloading:  77%|███████▋  | 438M\/571M [00:08<00:02, 53.9MB\/s]\rDownloading:  78%|███████▊  | 443M\/571M [00:08<00:02, 53.8MB\/s]\rDownloading:  79%|███████▊  | 448M\/571M [00:08<00:02, 54.9MB\/s]\rDownloading:  80%|███████▉  | 454M\/571M [00:09<00:02, 55.6MB\/s]\rDownloading:  80%|████████  | 459M\/571M [00:09<00:02, 56.2MB\/s]\rDownloading:  81%|████████▏ | 465M\/571M [00:09<00:01, 56.6MB\/s]\rDownloading:  82%|████████▏ | 470M\/571M [00:09<00:01, 57.1MB\/s]\rDownloading:  83%|████████▎ | 476M\/571M [00:09<00:01, 56.7MB\/s]\rDownloading:  84%|████████▍ | 481M\/571M [00:09<00:01, 55.3MB\/s]\rDownloading:  85%|████████▌ | 487M\/571M [00:09<00:01, 53.2MB\/s]\rDownloading:  86%|████████▌ | 492M\/571M [00:09<00:01, 54.4MB\/s]\rDownloading:  87%|████████▋ | 497M\/571M [00:09<00:01, 54.1MB\/s]\rDownloading:  88%|████████▊ | 502M\/571M [00:09<00:01, 53.9MB\/s]\rDownloading:  89%|████████▉ | 508M\/571M [00:10<00:01, 53.3MB\/s]\rDownloading:  90%|████████▉ | 513M\/571M [00:10<00:01, 53.4MB\/s]\rDownloading:  91%|█████████ | 518M\/571M [00:10<00:01, 54.9MB\/s]\rDownloading:  92%|█████████▏| 524M\/571M [00:10<00:00, 55.6MB\/s]\rDownloading:  93%|█████████▎| 529M\/571M [00:10<00:00, 55.0MB\/s]\rDownloading:  94%|█████████▎| 534M\/571M [00:10<00:00, 54.3MB\/s]\rDownloading:  95%|█████████▍| 540M\/571M [00:10<00:00, 55.3MB\/s]\rDownloading:  95%|█████████▌| 545M\/571M [00:10<00:00, 54.4MB\/s]\rDownloading:  96%|█████████▋| 550M\/571M [00:10<00:00, 53.8MB\/s]\rDownloading:  97%|█████████▋| 556M\/571M [00:10<00:00, 54.1MB\/s]\rDownloading:  98%|█████████▊| 561M\/571M [00:11<00:00, 54.4MB\/s]\rDownloading:  99%|█████████▉| 566M\/571M [00:11<00:00, 54.5MB\/s]\rDownloading: 100%|██████████| 571M\/571M [00:11<00:00, 53.1MB\/s]\n",
      "\rDownloading:   0%|          | 0.00\/665 [00:00<?, ?B\/s]\rDownloading: 100%|██████████| 665\/665 [00:00<00:00, 3.19MB\/s]\n",
      "\rDownloading:   0%|          | 0.00\/523M [00:00<?, ?B\/s]\rDownloading:   1%|          | 5.39M\/523M [00:00<00:09, 56.5MB\/s]\rDownloading:   2%|▏         | 10.8M\/523M [00:00<00:10, 53.0MB\/s]\rDownloading:   3%|▎         | 15.8M\/523M [00:00<00:10, 51.5MB\/s]\rDownloading:   4%|▍         | 20.8M\/523M [00:00<00:10, 50.8MB\/s]\rDownloading:   5%|▍         | 25.6M\/523M [00:00<00:10, 50.5MB\/s]\rDownloading:   6%|▌         | 30.4M\/523M [00:00<00:10, 48.3MB\/s]\rDownloading:   7%|▋         | 35.1M\/523M [00:00<00:10, 48.6MB\/s]\rDownloading:   8%|▊         | 39.9M\/523M [00:00<00:10, 49.0MB\/s]\rDownloading:   9%|▊         | 44.7M\/523M [00:00<00:10, 49.4MB\/s]\rDownloading:   9%|▉         | 49.6M\/523M [00:01<00:09, 50.0MB\/s]\rDownloading:  10%|█         | 54.5M\/523M [00:01<00:09, 50.4MB\/s]\rDownloading:  11%|█▏        | 59.4M\/523M [00:01<00:09, 50.7MB\/s]\rDownloading:  12%|█▏        | 64.3M\/523M [00:01<00:09, 50.9MB\/s]\rDownloading:  13%|█▎        | 69.2M\/523M [00:01<00:09, 51.1MB\/s]\rDownloading:  14%|█▍        | 74.1M\/523M [00:01<00:09, 50.9MB\/s]\rDownloading:  15%|█▌        | 78.9M\/523M [00:01<00:09, 49.6MB\/s]\rDownloading:  16%|█▌        | 83.9M\/523M [00:01<00:09, 50.2MB\/s]\rDownloading:  17%|█▋        | 88.8M\/523M [00:01<00:08, 50.7MB\/s]\rDownloading:  18%|█▊        | 94.2M\/523M [00:01<00:08, 52.5MB\/s]\rDownloading:  19%|█▉        | 99.6M\/523M [00:02<00:08, 53.7MB\/s]\rDownloading:  20%|██        | 105M\/523M [00:02<00:08, 54.6MB\/s] \rDownloading:  21%|██        | 110M\/523M [00:02<00:07, 55.3MB\/s]\rDownloading:  22%|██▏       | 116M\/523M [00:02<00:07, 55.6MB\/s]\rDownloading:  23%|██▎       | 121M\/523M [00:02<00:07, 55.7MB\/s]\rDownloading:  24%|██▍       | 127M\/523M [00:02<00:07, 56.1MB\/s]\rDownloading:  25%|██▌       | 132M\/523M [00:02<00:07, 56.1MB\/s]\rDownloading:  26%|██▋       | 137M\/523M [00:02<00:07, 56.2MB\/s]\rDownloading:  27%|██▋       | 143M\/523M [00:02<00:07, 56.2MB\/s]\rDownloading:  28%|██▊       | 148M\/523M [00:02<00:06, 56.4MB\/s]\rDownloading:  29%|██▉       | 154M\/523M [00:03<00:06, 56.5MB\/s]\rDownloading:  30%|███       | 159M\/523M [00:03<00:06, 56.6MB\/s]\rDownloading:  31%|███▏      | 164M\/523M [00:03<00:06, 56.6MB\/s]\rDownloading:  32%|███▏      | 170M\/523M [00:03<00:06, 56.4MB\/s]\rDownloading:  34%|███▎      | 175M\/523M [00:03<00:06, 56.2MB\/s]\rDownloading:  35%|███▍      | 181M\/523M [00:03<00:06, 56.4MB\/s]\rDownloading:  36%|███▌      | 186M\/523M [00:03<00:06, 56.7MB\/s]\rDownloading:  37%|███▋      | 191M\/523M [00:03<00:06, 54.5MB\/s]\rDownloading:  38%|███▊      | 197M\/523M [00:03<00:06, 53.2MB\/s]\rDownloading:  39%|███▊      | 202M\/523M [00:03<00:06, 52.8MB\/s]\rDownloading:  40%|███▉      | 207M\/523M [00:04<00:06, 52.5MB\/s]\rDownloading:  41%|████      | 212M\/523M [00:04<00:06, 52.1MB\/s]\rDownloading:  41%|████▏     | 217M\/523M [00:04<00:06, 51.6MB\/s]\rDownloading:  42%|████▏     | 222M\/523M [00:04<00:06, 51.1MB\/s]\rDownloading:  43%|████▎     | 227M\/523M [00:04<00:06, 50.7MB\/s]\rDownloading:  44%|████▍     | 231M\/523M [00:04<00:06, 50.5MB\/s]\rDownloading:  45%|████▌     | 236M\/523M [00:04<00:05, 50.8MB\/s]\rDownloading:  46%|████▌     | 241M\/523M [00:04<00:05, 50.9MB\/s]\rDownloading:  47%|████▋     | 246M\/523M [00:04<00:05, 51.2MB\/s]\rDownloading:  48%|████▊     | 251M\/523M [00:04<00:05, 51.2MB\/s]\rDownloading:  49%|████▉     | 256M\/523M [00:05<00:05, 51.3MB\/s]\rDownloading:  50%|████▉     | 261M\/523M [00:05<00:05, 51.2MB\/s]\rDownloading:  51%|█████     | 266M\/523M [00:05<00:05, 51.2MB\/s]\rDownloading:  52%|█████▏    | 271M\/523M [00:05<00:05, 51.2MB\/s]\rDownloading:  53%|█████▎    | 276M\/523M [00:05<00:05, 51.4MB\/s]\rDownloading:  54%|█████▎    | 280M\/523M [00:05<00:04, 51.5MB\/s]\rDownloading:  55%|█████▍    | 285M\/523M [00:05<00:04, 51.0MB\/s]\rDownloading:  56%|█████▌    | 291M\/523M [00:05<00:04, 52.7MB\/s]\rDownloading:  57%|█████▋    | 296M\/523M [00:05<00:04, 53.5MB\/s]\rDownloading:  58%|█████▊    | 301M\/523M [00:05<00:04, 54.2MB\/s]\rDownloading:  59%|█████▊    | 307M\/523M [00:06<00:04, 54.8MB\/s]\rDownloading:  60%|█████▉    | 312M\/523M [00:06<00:03, 55.3MB\/s]\rDownloading:  61%|██████    | 318M\/523M [00:06<00:03, 55.5MB\/s]\rDownloading:  62%|██████▏   | 323M\/523M [00:06<00:03, 55.7MB\/s]\rDownloading:  63%|██████▎   | 328M\/523M [00:06<00:03, 55.9MB\/s]\rDownloading:  64%|██████▍   | 334M\/523M [00:06<00:03, 56.0MB\/s]\rDownloading:  65%|██████▍   | 339M\/523M [00:06<00:03, 56.0MB\/s]\rDownloading:  66%|██████▌   | 344M\/523M [00:06<00:03, 56.2MB\/s]\rDownloading:  67%|██████▋   | 350M\/523M [00:06<00:03, 56.1MB\/s]\rDownloading:  68%|██████▊   | 355M\/523M [00:06<00:03, 56.2MB\/s]\rDownloading:  69%|██████▉   | 360M\/523M [00:07<00:03, 56.2MB\/s]\rDownloading:  70%|██████▉   | 366M\/523M [00:07<00:02, 56.3MB\/s]\rDownloading:  71%|███████   | 371M\/523M [00:07<00:02, 56.4MB\/s]\rDownloading:  72%|███████▏  | 377M\/523M [00:07<00:02, 56.5MB\/s]\rDownloading:  73%|███████▎  | 382M\/523M [00:07<00:02, 56.4MB\/s]\rDownloading:  74%|███████▍  | 387M\/523M [00:07<00:02, 56.4MB\/s]\rDownloading:  75%|███████▌  | 393M\/523M [00:07<00:02, 56.4MB\/s]\rDownloading:  76%|███████▌  | 398M\/523M [00:07<00:02, 56.4MB\/s]\rDownloading:  77%|███████▋  | 404M\/523M [00:07<00:02, 56.4MB\/s]\rDownloading:  78%|███████▊  | 409M\/523M [00:07<00:02, 56.5MB\/s]\rDownloading:  79%|███████▉  | 414M\/523M [00:08<00:02, 56.5MB\/s]\rDownloading:  80%|████████  | 420M\/523M [00:08<00:01, 56.5MB\/s]\rDownloading:  81%|████████▏ | 425M\/523M [00:08<00:01, 56.5MB\/s]\rDownloading:  82%|████████▏ | 431M\/523M [00:08<00:01, 56.4MB\/s]\rDownloading:  83%|████████▎ | 436M\/523M [00:08<00:01, 56.2MB\/s]\rDownloading:  84%|████████▍ | 441M\/523M [00:08<00:01, 56.4MB\/s]\rDownloading:  85%|████████▌ | 447M\/523M [00:08<00:01, 56.5MB\/s]\rDownloading:  86%|████████▋ | 452M\/523M [00:08<00:01, 56.6MB\/s]\rDownloading:  88%|████████▊ | 458M\/523M [00:08<00:01, 56.6MB\/s]\rDownloading:  89%|████████▊ | 463M\/523M [00:08<00:01, 56.6MB\/s]\rDownloading:  90%|████████▉ | 468M\/523M [00:09<00:01, 56.6MB\/s]\rDownloading:  91%|█████████ | 474M\/523M [00:09<00:00, 56.8MB\/s]\rDownloading:  92%|█████████▏| 479M\/523M [00:09<00:00, 56.6MB\/s]\rDownloading:  93%|█████████▎| 485M\/523M [00:09<00:00, 56.6MB\/s]\rDownloading:  94%|█████████▎| 490M\/523M [00:09<00:00, 56.6MB\/s]\rDownloading:  95%|█████████▍| 495M\/523M [00:09<00:00, 56.6MB\/s]\rDownloading:  96%|█████████▌| 501M\/523M [00:09<00:00, 56.4MB\/s]\rDownloading:  97%|█████████▋| 506M\/523M [00:09<00:00, 56.5MB\/s]\rDownloading:  98%|█████████▊| 512M\/523M [00:09<00:00, 56.6MB\/s]\rDownloading:  99%|█████████▉| 517M\/523M [00:10<00:00, 56.6MB\/s]\rDownloading: 100%|█████████▉| 523M\/523M [00:10<00:00, 56.7MB\/s]\rDownloading: 100%|██████████| 523M\/523M [00:10<00:00, 54.2MB\/s]\n",
      "\rDownloading:   0%|          | 0.00\/905 [00:00<?, ?B\/s]\rDownloading: 100%|██████████| 905\/905 [00:00<00:00, 4.31MB\/s]\n",
      "\rDownloading:   0%|          | 0.00\/939k [00:00<?, ?B\/s]\rDownloading: 100%|██████████| 939k\/939k [00:00<00:00, 41.8MB\/s]\n",
      "\rDownloading:   0%|          | 0.00\/512k [00:00<?, ?B\/s]\rDownloading: 100%|██████████| 512k\/512k [00:00<00:00, 7.45MB\/s]\n",
      "\rDownloading:   0%|          | 0.00\/2.12M [00:00<?, ?B\/s]\rDownloading:  24%|██▎       | 513k\/2.12M [00:00<00:00, 4.58MB\/s]\rDownloading:  44%|████▍     | 961k\/2.12M [00:00<00:00, 4.43MB\/s]\rDownloading:  90%|████████▉ | 1.91M\/2.12M [00:00<00:00, 6.56MB\/s]\rDownloading: 100%|██████████| 2.12M\/2.12M [00:00<00:00, 6.38MB\/s]\n",
      "\rDownloading:   0%|          | 0.00\/389 [00:00<?, ?B\/s]\rDownloading: 100%|██████████| 389\/389 [00:00<00:00, 1.79MB\/s]\n",
      "\rDownloading:   0%|          | 0.00\/26.0 [00:00<?, ?B\/s]\rDownloading: 100%|██████████| 26.0\/26.0 [00:00<00:00, 122kB\/s]\n",
      "\rDownloading:   0%|          | 0.00\/0.99M [00:00<?, ?B\/s]\rDownloading: 100%|██████████| 0.99M\/0.99M [00:00<00:00, 15.6MB\/s]\n",
      "\rDownloading:   0%|          | 0.00\/446k [00:00<?, ?B\/s]\rDownloading:  58%|█████▊    | 257k\/446k [00:00<00:00, 1.99MB\/s]\rDownloading: 100%|██████████| 446k\/446k [00:00<00:00, 3.33MB\/s]\n",
      "\rDownloading:   0%|          | 0.00\/1.29M [00:00<?, ?B\/s]\rDownloading:   7%|▋         | 97.0k\/1.29M [00:00<00:01, 744kB\/s]\rDownloading:  39%|███▉      | 513k\/1.29M [00:00<00:00, 2.16MB\/s]\rDownloading: 100%|██████████| 1.29M\/1.29M [00:00<00:00, 3.98MB\/s]\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "\/opt\/python\/envs\/minimal\/lib\/python3.8\/site-packages\/transformers\/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"mj4GFFArORgzUqLpkqj4Xn",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "pengi_output_prompts = generated_responses[0][0]\n",
    "pengi_output_prompts"
   ],
   "execution_count":8,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "[' a violin is playing a note f. ',\n",
       " ' a violin is playing a melody. ',\n",
       " ' a violin is playing a violin. ',\n",
       " ' a violin is playing a note f#. ',\n",
       " ' a violin is playing a piece of a violin. ',\n",
       " ' a violin is playing a piece of a violin ']"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"vCOTMpO2xa7elXVBy5Cwd6",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "from sentence_transformers import SentenceTransformer"
   ],
   "execution_count":5,
   "outputs":[
    
   ],
   "metadata":{
    "datalore":{
     "node_id":"54VPhPpPmQJct7PClwRQj2",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "goal_prompt = \"A robotic sounding violin\"\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "goal_encoded = model.encode(goal_prompt)\n",
    "\n",
    "for prompt in pengi_output_prompts:\n",
    "    prompt_encoded = model.encode(prompt)\n",
    "    # new_encoded_prompt = prompt_encoded + (goal_encoded - prompt_encoded)\n"
   ],
   "execution_count":10,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n",
      "(384,)\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"XWaVKiykMtj3Jgp2OYd48Q",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "\"\"\"make variations of input image\"\"\"\n",
    "import os, sys\n",
    "os.chdir(\"\/data\/notebook_files\/Make-An-Audio-main\")\n",
    "\n",
    "print(os.path.abspath(\".\"))\n",
    "import argparse, os, sys, glob\n",
    "import PIL\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange, repeat\n",
    "from torchvision.utils import make_grid\n",
    "from torch import autocast\n",
    "import librosa\n",
    "# from contextlib import nullcontext\n",
    "import time\n",
    "from pytorch_lightning import seed_everything\n",
    "import math\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from vocoder.bigvgan.models import VocoderBigVGAN\n",
    "# from ldm.data.extract_mel_spectrogram import TRANSFORMS_22050,TRANSFORMS_16000\n",
    "from preprocess.NAT_mel import MelNet\n",
    "import soundfile\n",
    "\n",
    "batch_max_length = 624\n",
    "SAMPLE_RATE= 16000\n",
    "\n",
    "def chunk(it, size):\n",
    "    it = iter(it)\n",
    "    return iter(lambda: tuple(islice(it, size)), ())\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=True):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    if \"global_step\" in pl_sd:\n",
    "        print(f\"Global Step: {pl_sd['global_step']}\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_audio(path,transform,sr=16000,batch_max_length=624):# load wav and return mel\n",
    "    wav,_ = librosa.load(path,sr=sr)\n",
    "\n",
    "    audio = transform(wav) # (1,melbins,T)\n",
    "    if audio.shape[2] <= batch_max_length:\n",
    "        n_repeat = math.ceil((batch_max_length + 1) \/ audio.shape[1])\n",
    "        audio = audio.repeat(1,1, n_repeat)\n",
    "\n",
    "    audio = audio[..., :batch_max_length].unsqueeze(0) # shape [1,1,80,batch_max_length]\n",
    "    return audio\n",
    "\n",
    "def load_img(path):# load mel\n",
    "    audio = np.load(path)\n",
    "    if audio.shape[1] <= batch_max_length:\n",
    "        n_repeat = math.ceil((batch_max_length + 1) \/ audio.shape[1])\n",
    "        audio = np.tile(audio, reps=(1, n_repeat))\n",
    "\n",
    "    audio = audio[:, :batch_max_length]\n",
    "    audio = torch.FloatTensor(audio)[None, None, :, :] # [1,1,80,batch_max_length]\n",
    "    return audio\n",
    "\n",
    "\n",
    "class Opt:\n",
    "    def __init__(self):\n",
    "        self.config = \"\/data\/notebook_files\/Make-An-Audio-main\/configs\/text_to_audio\/txt2audio_args.yaml\"\n",
    "        self.ckpt = \"\/data\/notebook_files\/Make-An-Audio-main\/useful_ckpts\/maa1_full.ckpt\"\n",
    "        self.vocoder_ckpt = \"\/data\/notebook_files\/Make-An-Audio-main\/useful_ckpts\/bigvnat\"\n",
    "        self.outdir = \"\/data\/notebook_files\/Make-An-Audio-main\/outputs\/audio2audio-samples\"\n",
    "        self.from_file = None\n",
    "        \n",
    "        self.strength = 0.3\n",
    "        self.seed = 42\n",
    "        self.scale = 3.0\n",
    "        self.n_samples = 2\n",
    "        self.n_iter = 1\n",
    "\n",
    "        self.ddim_steps = 100\n",
    "        self.ddim_eta = 0.0\n",
    "\n",
    "        self.prompt = \"a bird chirping\"\n",
    "        self.init_audio = \"\/data\/notebook_files\/violin_0.wav\"\n",
    "\n",
    "def main():\n",
    "    opt = Opt()\n",
    "    seed_everything(opt.seed)\n",
    "\n",
    "    config = OmegaConf.load(f\"{opt.config}\")\n",
    "    model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
    "\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    hparams = {\n",
    "        'audio_sample_rate': SAMPLE_RATE,\n",
    "        'audio_num_mel_bins':80,\n",
    "        'fft_size': 1024,\n",
    "        'win_size': 1024,\n",
    "        'hop_size': 256,\n",
    "        'fmin': 0,\n",
    "        'fmax': 8000,\n",
    "        'batch_max_length': 1248, \n",
    "        'mode': 'pad', # pad,none,\n",
    "    }\n",
    "    melnet = MelNet(hparams)\n",
    "    sampler = DDIMSampler(model)\n",
    "    vocoder = VocoderBigVGAN(opt.vocoder_ckpt,device)\n",
    "\n",
    "    os.makedirs(opt.outdir, exist_ok=True)\n",
    "    outpath = opt.outdir\n",
    "\n",
    "    batch_size = opt.n_samples # 一个prompt产生n_samples个结果\n",
    "    if not opt.from_file: # load prompts from this file\n",
    "        prompt = opt.prompt\n",
    "        assert prompt is not None\n",
    "        data = [batch_size * [prompt]]\n",
    "    else:\n",
    "        print(f\"reading prompts from {opt.from_file}\")\n",
    "        with open(opt.from_file, \"r\") as f:\n",
    "            data = f.read().splitlines()\n",
    "            data = list(chunk(data, batch_size))\n",
    "\n",
    "\n",
    "    sample_path = os.path.join(outpath, \"samples\")\n",
    "    os.makedirs(sample_path, exist_ok=True)\n",
    "    base_count = len(os.listdir(sample_path))\n",
    "\n",
    "    assert os.path.isfile(opt.init_audio)\n",
    "    init_image = load_audio(opt.init_audio,transform=melnet).to(device)\n",
    "    init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "    init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
    "    sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
    "\n",
    "    assert 0. <= opt.strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "    t_enc = int(opt.strength * opt.ddim_steps)\n",
    "    print(f\"target t_enc is {t_enc} steps\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with model.ema_scope():\n",
    "            tic = time.time()\n",
    "            all_samples = list()\n",
    "            for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
    "                for prompts in tqdm(data, desc=\"data\"):\n",
    "                    uc = None\n",
    "                    if opt.scale != 1.0: # default=5.0\n",
    "                        uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                    if isinstance(prompts, tuple):\n",
    "                        prompts = list(prompts)\n",
    "                    c = model.get_learned_conditioning(prompts)\n",
    "                    z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device)) # [B, channel, c, h]\n",
    "                    # decode it\n",
    "                    samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
    "                                                unconditional_conditioning=uc,)\n",
    "\n",
    "                    x_samples = model.decode_first_stage(samples)\n",
    "                    print(x_samples.shape)\n",
    "                    for x_sample in x_samples:\n",
    "                        spec = x_sample[0].cpu().numpy()\n",
    "                        spec_ori = init_image[0][0].cpu().numpy()\n",
    "                        print(x_sample.shape,spec.shape,init_image.shape)\n",
    "                        wav = vocoder.vocode(spec)\n",
    "                        wav_ori = vocoder.vocode(spec_ori)\n",
    "                        soundfile.write(os.path.join(outpath, f'{prompt.replace(\" \", \"-\")}.wav'), wav, SAMPLE_RATE, 'FLOAT')\n",
    "                        soundfile.write(os.path.join(outpath, f'{prompt.replace(\" \", \"-\")}_ori.wav'), wav_ori, SAMPLE_RATE, 'FLOAT')\n",
    "                        base_count += 1\n",
    "                    all_samples.append(x_samples)\n",
    "\n",
    "\n",
    "    print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
    "            f\" \\nEnjoy.\")\n"
   ],
   "execution_count":3,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "\/data\/notebook_files\/Make-An-Audio-main\n",
      "Loading model from \/data\/notebook_files\/Make-An-Audio-main\/useful_ckpts\/maa1_full.ckpt\n",
      "Global Step: 3722264\n",
      "LatentDiffusion_audio: Running in eps-prediction mode\n",
      "DiffusionWrapper has 160.22 M params.\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 78, 78) = 24336 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "TextEncoder comes with 111.32 M params.\n",
      "target t_enc is 30 steps\n",
      "torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "Your samples are ready and waiting for you here: \n",
      "\/data\/notebook_files\/Make-An-Audio-main\/outputs\/audio2audio-samples \n",
      " \n",
      "Enjoy.\n"
     ],
     "output_type":"stream"
    },
    {
     "name":"stderr",
     "text":[
      "Global seed set to 42\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\/opt\/python\/envs\/minimal\/lib\/python3.8\/site-packages\/torch\/functional.py:641: UserWarning: stft with return_complex=False is deprecated. In a future pytorch release, stft will return complex tensors for all inputs, and return_complex=False will raise an error.\n",
      "Note: you can still call torch.view_as_real on the complex output to recover the old return format. (Triggered internally at ..\/aten\/src\/ATen\/native\/SpectralOps.cpp:862.)\n",
      "  return _VF.stft(input, n_fft, hop_length, win_length, window,  # type: ignore[attr-defined]\n",
      "\rSampling:   0%|          | 0\/1 [00:00<?, ?it\/s]\n",
      "\rdata:   0%|          | 0\/1 [00:00<?, ?it\/s]\u001b[A\n",
      "\rdata: 100%|██████████| 1\/1 [00:06<00:00,  6.55s\/it]\u001b[A\rdata: 100%|██████████| 1\/1 [00:06<00:00,  6.55s\/it]\n",
      "\rSampling: 100%|██████████| 1\/1 [00:06<00:00,  6.55s\/it]\rSampling: 100%|██████████| 1\/1 [00:06<00:00,  6.55s\/it]\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"ddRhJPVZOXqDAfXgQbj8bL",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "opt = Opt()\n",
    "seed_everything(opt.seed)\n",
    "\n",
    "config = OmegaConf.load(f\"{opt.config}\")\n",
    "model = load_model_from_config(config, f\"{opt.ckpt}\")\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "hparams = {\n",
    "    'audio_sample_rate': SAMPLE_RATE,\n",
    "    'audio_num_mel_bins':80,\n",
    "    'fft_size': 1024,\n",
    "    'win_size': 1024,\n",
    "    'hop_size': 256,\n",
    "    'fmin': 0,\n",
    "    'fmax': 8000,\n",
    "    'batch_max_length': 1248, \n",
    "    'mode': 'pad', # pad,none,\n",
    "}\n",
    "melnet = MelNet(hparams)\n",
    "sampler = DDIMSampler(model)\n",
    "vocoder = VocoderBigVGAN(opt.vocoder_ckpt,device)\n",
    "\n",
    "os.makedirs(opt.outdir, exist_ok=True)\n",
    "outpath = opt.outdir\n",
    "\n",
    "batch_size = opt.n_samples # 一个prompt产生n_samples个结果\n",
    "if not opt.from_file: # load prompts from this file\n",
    "    prompt = opt.prompt\n",
    "    assert prompt is not None\n",
    "    data = [batch_size * [prompt]]\n",
    "else:\n",
    "    print(f\"reading prompts from {opt.from_file}\")\n",
    "    with open(opt.from_file, \"r\") as f:\n",
    "        data = f.read().splitlines()\n",
    "        data = list(chunk(data, batch_size))\n",
    "\n",
    "\n",
    "sample_path = os.path.join(outpath, \"samples\")\n",
    "os.makedirs(sample_path, exist_ok=True)\n",
    "base_count = len(os.listdir(sample_path))\n",
    "\n",
    "assert os.path.isfile(opt.init_audio)\n",
    "init_image = load_audio(opt.init_audio,transform=melnet).to(device)\n",
    "init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)\n",
    "init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space\n",
    "sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)\n",
    "\n",
    "assert 0. <= opt.strength <= 1., 'can only work with strength in [0.0, 1.0]'\n",
    "t_enc = int(opt.strength * opt.ddim_steps)\n",
    "print(f\"target t_enc is {t_enc} steps\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    with model.ema_scope():\n",
    "        tic = time.time()\n",
    "        all_samples = list()\n",
    "        for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
    "            for prompts in tqdm(data, desc=\"data\"):\n",
    "                uc = None\n",
    "                \n",
    "                if opt.scale != 1.0: # default=5.0\n",
    "                    uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                if isinstance(prompts, tuple):\n",
    "                    prompts = list(prompts)\n",
    "                c = model.get_learned_conditioning(prompts)\n",
    "                # c = model.get_learned_conditioning(prompts) + model.get_learned_conditioning(goal) - model.get_learned_conditioning(prompt)\n",
    "                z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device)) # [B, channel, c, h]\n",
    "                # decode it\n",
    "                samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,\n",
    "                                            unconditional_conditioning=uc,)\n",
    "\n",
    "                x_samples = model.decode_first_stage(samples)\n",
    "                print(x_samples.shape)\n",
    "                for x_sample in x_samples:\n",
    "                    spec = x_sample[0].cpu().numpy()\n",
    "                    spec_ori = init_image[0][0].cpu().numpy()\n",
    "                    print(x_sample.shape,spec.shape,init_image.shape)\n",
    "                    wav = vocoder.vocode(spec)\n",
    "                    wav_ori = vocoder.vocode(spec_ori)\n",
    "                    soundfile.write(os.path.join(outpath, f'{prompt.replace(\" \", \"-\")}.wav'), wav, SAMPLE_RATE, 'FLOAT')\n",
    "                    soundfile.write(os.path.join(outpath, f'{prompt.replace(\" \", \"-\")}_ori.wav'), wav_ori, SAMPLE_RATE, 'FLOAT')\n",
    "                    base_count += 1\n",
    "                all_samples.append(x_samples)\n",
    "\n",
    "\n",
    "print(f\"Your samples are ready and waiting for you here: \\n{outpath} \\n\"\n",
    "        f\" \\nEnjoy.\")"
   ],
   "execution_count":4,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "Loading model from \/data\/notebook_files\/Make-An-Audio-main\/useful_ckpts\/maa1_full.ckpt\n",
      "Global Step: 3722264\n",
      "LatentDiffusion_audio: Running in eps-prediction mode\n",
      "DiffusionWrapper has 160.22 M params.\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "Working with z of shape (1, 4, 78, 78) = 24336 dimensions.\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 512 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "making attention of type 'vanilla' with 256 in_channels\n",
      "TextEncoder comes with 111.32 M params.\n",
      "target t_enc is 30 steps\n",
      "torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "torch.Size([1, 80, 624]) (80, 624) torch.Size([2, 1, 80, 624])\n",
      "Your samples are ready and waiting for you here: \n",
      "\/data\/notebook_files\/Make-An-Audio-main\/outputs\/audio2audio-samples \n",
      " \n",
      "Enjoy.\n"
     ],
     "output_type":"stream"
    },
    {
     "name":"stderr",
     "text":[
      "Global seed set to 42\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "\rSampling:   0%|          | 0\/1 [00:00<?, ?it\/s]\n",
      "\rdata:   0%|          | 0\/1 [00:00<?, ?it\/s]\u001b[A\n",
      "\rdata: 100%|██████████| 1\/1 [00:06<00:00,  6.63s\/it]\u001b[A\rdata: 100%|██████████| 1\/1 [00:06<00:00,  6.63s\/it]\n",
      "\rSampling: 100%|██████████| 1\/1 [00:06<00:00,  6.64s\/it]\rSampling: 100%|██████████| 1\/1 [00:06<00:00,  6.64s\/it]\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"4rYMCipUAMGy8T3jfgLqTK",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "print(model.cond_stage_model)"
   ],
   "execution_count":15,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "FrozenCLAPEmbedder(\n",
      "  (caption_encoder): TextEncoder(\n",
      "    (base): BertModel(\n",
      "      (embeddings): BertEmbeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (token_type_embeddings): Embedding(2, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (encoder): BertEncoder(\n",
      "        (layer): ModuleList(\n",
      "          (0-11): 12 x BertLayer(\n",
      "            (attention): BertAttention(\n",
      "              (self): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (output): BertSelfOutput(\n",
      "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (intermediate): BertIntermediate(\n",
      "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (intermediate_act_fn): GELUActivation()\n",
      "            )\n",
      "            (output): BertOutput(\n",
      "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "    )\n",
      "    (projection): Projection(\n",
      "      (linear1): Linear(in_features=768, out_features=1024, bias=False)\n",
      "      (linear2): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (drop): Dropout(p=0.5, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"U5ag00SplvhMahTBwot3Hn",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "c"
   ],
   "execution_count":18,
   "outputs":[
    {
     "data":{
      "text\/plain":[
       "tensor([[[ 0.5392, -1.1894,  0.9112,  ..., -1.7570,  0.4956, -0.9635],\n",
       "         [-0.4502, -1.1678,  1.1993,  ...,  0.6420,  2.6234, -0.5755],\n",
       "         [-1.1369, -1.6566,  0.3870,  ...,  0.2241,  1.6512,  1.1209],\n",
       "         ...,\n",
       "         [ 1.9683,  0.5220, -0.0055,  ..., -1.1538, -0.7219,  0.2007],\n",
       "         [ 1.9511,  0.8519, -0.1219,  ..., -1.0268, -0.8133,  0.2305],\n",
       "         [ 2.0007,  0.7828, -0.3059,  ..., -1.1670, -0.9217,  0.0657]],\n",
       "\n",
       "        [[ 0.5392, -1.1894,  0.9112,  ..., -1.7570,  0.4956, -0.9635],\n",
       "         [-0.4502, -1.1678,  1.1993,  ...,  0.6420,  2.6234, -0.5755],\n",
       "         [-1.1369, -1.6566,  0.3870,  ...,  0.2241,  1.6512,  1.1209],\n",
       "         ...,\n",
       "         [ 1.9683,  0.5220, -0.0055,  ..., -1.1538, -0.7219,  0.2007],\n",
       "         [ 1.9511,  0.8519, -0.1219,  ..., -1.0268, -0.8133,  0.2305],\n",
       "         [ 2.0007,  0.7828, -0.3059,  ..., -1.1670, -0.9217,  0.0657]]],\n",
       "       device='cuda:0')"
      ]
     },
     "metadata":{
      
     },
     "output_type":"display_data"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"n6tjZRZMtcq0bBYZZsyebY",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  },
  {
   "cell_type":"code",
   "source":[
    "print(model.cond_stage_model.caption_encoder)"
   ],
   "execution_count":17,
   "outputs":[
    {
     "name":"stdout",
     "text":[
      "TextEncoder(\n",
      "  (base): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (projection): Projection(\n",
      "    (linear1): Linear(in_features=768, out_features=1024, bias=False)\n",
      "    (linear2): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (drop): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ],
     "output_type":"stream"
    }
   ],
   "metadata":{
    "datalore":{
     "node_id":"Yzi6J72TEMdXTa3T92nwhr",
     "type":"CODE",
     "hide_input_from_viewers":true,
     "hide_output_from_viewers":true
    }
   }
  }
 ],
 "metadata":{
  "kernelspec":{
   "display_name":"Python",
   "language":"python",
   "name":"python"
  },
  "datalore":{
   "computation_mode":"JUPYTER",
   "package_manager":"pip",
   "base_environment":"minimal",
   "packages":[
    {
     "name":"librosa",
     "version":"0.10.0.post2",
     "source":"PIP"
    },
    {
     "name":"PyYAML",
     "version":"6.0",
     "source":"PIP"
    },
    {
     "name":"PyYAML",
     "version":"6.0",
     "source":"PIP"
    },
    {
     "name":"torch",
     "version":"2.0.1",
     "source":"PIP"
    },
    {
     "name":"torch-fidelity",
     "version":"0.3.0",
     "source":"PIP"
    },
    {
     "name":"scipy",
     "source":"PIP"
    },
    {
     "name":"importlib_resources",
     "version":"5.12.0",
     "source":"PIP"
    },
    {
     "name":"torchaudio>=0.13.0",
     "source":"PIP"
    },
    {
     "name":"torchvision>=0.14.0",
     "source":"PIP"
    },
    {
     "name":"tqdm",
     "source":"PIP"
    },
    {
     "name":"omegaconf",
     "source":"PIP"
    },
    {
     "name":"einops",
     "source":"PIP"
    },
    {
     "name":"numpy<=1.23.5",
     "source":"PIP"
    },
    {
     "name":"soundfile",
     "source":"PIP"
    },
    {
     "name":"pandas",
     "source":"PIP"
    },
    {
     "name":"torchlibrosa ",
     "version":" 0.1.0",
     "source":"PIP"
    },
    {
     "name":"transformers",
     "version":"4.18.0",
     "source":"PIP"
    },
    {
     "name":"ftfy",
     "source":"PIP"
    },
    {
     "name":"pytorch-lightning",
     "version":"1.7.0",
     "source":"PIP"
    },
    {
     "name":"torchmetrics",
     "version":"0.11.1",
     "source":"PIP"
    },
    {
     "name":"sentence-transformers",
     "source":"PIP"
    },
    {
     "name":"git+https:\/\/github.com\/CompVis\/taming-transformers.git@master#egg=taming-transformers",
     "source":"PIP"
    }
   ],
   "report_row_ids":[
    
   ],
   "version":3
  }
 },
 "nbformat":4,
 "nbformat_minor":4
}